Law of large numbers
	A random experiment is repeated $n$ times
	$\tilde{f_n}(E)$ is the relative frequency of event $E$
	$\underset{n\to \infty}{lim} \tilde{f_n}(E)$ fluctuates less

RNG
	Can be generated by sampling noise signals from the environment
	$z$ is a generated random number
	Ideally $P(z \in (0, 1/2]) = P(z \in (1/2, 1))$
	For an interval $(a, b)$
		$P(z \in (a, b)) = b - a$
	For a CLG the period (Number of iterations without repetition) is $m-1$
	Good sample values are
		$m = 2^{31} -1$
		$a = 16807$

Tests for random numbers
	$H_0 :$ Generated random numbers are uniformly distributed on $(0, 1)$
	Perform hypothesis test

Test for uniformity
	Divide unit interval into $K$ subintervals
	$\#z$ in any subinterval is $E_i = n/K$
	Count the observed $\# z$ in each subinterval $O_i$
	$\mathscr{X}^2 = \overset{K}{\sum} \dfrac{(O_i - E_i)^2}{E_i}$
		$K-1$ degrees of freedom

Experiment
	Act or process of observation leading to a single unpredictable outcome 

Sample Point ($s_i$)
	Outcome of an experiment
	$p_i = P(s_i)$
	$\sum p_i = 1$

Probability Mass Function (PMF)
	$P(Y = y) = p(y)$

$Z = X + Y \to E(Z) = E(X) + E(Y)$
$E(aX + b) \to aE(X) + b$
$V(aX + b) \to a^2 V(X)$
$X$ and $Y$ are independent $\to E(XY) = E(X)E(Y)$
	Converse is not true!
$X$ and $Y$ are independent $\to V(X+Y) = V(X) + V(Y)$

Geometric Distribution
	$X$ has a geometric distribution with parameter $p$ if
		$P(X =k) = p(1-p)^{k-1}$
	$X \sim$ Geo($p$)
	$E(X) = \dfrac{1}{p}$
	$V(X) = \dfrac{1-p}{p^2}$
	$F(x) = 1 - (1-p)^x$

$P(X > s + t | X > t) = P(X > s)$
$P(X > s + t) = P(X > s) P(X > t)$

Exponential is memoryless
	$X \sim$ Exp($\lambda$) $\to P(X > t + s | X > t) = P(X > s)$

Markov Inequality
	What is the probability an r.v. $X$ exceeds some quantity
	$P(X > a) \leq \dfrac{E(X)}{a}$
	If $a$ is negative and the min value is negative then shift all values up to make the min 0
	Example:
		Given the expected time between stock crashes is 12 months, how likely is it the stock will go 60 months without a crash
		$P(X > 60) \leq \dfrac{12}{60} = \dfrac{1}{5}$

Chebyshev's Inequality
	$P(|X - E(X)| > c) \leq \dfrac{V(X)}{c^2}$

Tail Probabilities
	$X \sim N(\mu, \sigma^2) \to P(|X - \mu| > k\sigma)$ is small
	$P(|X - \mu| > k\sigma) \leq \dfrac{1}{k^2}$
	$P(|X - \mu| > k\sigma) \leq 2 \dfrac{exp(-k^2/2)}{k \sqrt{2\pi}}$

:=
	Assignment like in programming

Joint Distribution
	$p(x, y) := P((X = x) \cap (Y = y))$
	$p_X(x) := P(X = x)$
	$p_Y(x) := P(Y = y)$
	$p_X(x) = \sum p(x, y_i)$
	$p_Y(y) = \sum p(x_i, y)$
	$F(x, y) := P((X \leq x) \cap (Y \leq y)) = \int^x \int^y f(s, t)\ dt\ ds$

Marginal Density
	$f_X(x) = \int f(x, y) dy$